{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "covered-venezuela",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import afqinsight as afq\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from groupyr.transform import GroupExtractor\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "alive-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, Flatten, MaxPool1D, MaxPooling1D, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "verbal-craps",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, groups, feature_names, group_names, subjects, _ = afq.datasets.load_afq_data('../data/raw/age_data', target_cols=\"Age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fifth-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_extractor = GroupExtractor(select=['fa', 'md'], groups=groups, group_names=group_names)\n",
    "X = group_extractor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "included-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group_names = [g for g in group_names if g[0] in ['md', 'fa']]\n",
    "#print(len(group_names))\n",
    "#print(X.shape)\n",
    "#groups = groups[0:40]\n",
    "#group_extractor = GroupExtractor(select=['Right Cingulum Cingulate'], groups=groups, group_names=group_names)\n",
    "#X = group_extractor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "mental-evanescence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 4000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "drawn-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "written-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(strategy='median')\n",
    "imp.fit(X_train)\n",
    "X_train = imp.transform(X_train)\n",
    "X_test = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "proper-desktop",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57, 4000), (57,), (20, 4000), (20,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "armed-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reshaped = np.swapaxes(X_train.reshape((57, 40, 100)), 1, 2)\n",
    "X_test_reshaped = np.swapaxes(X_test.reshape((20, 40, 100)), 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "tutorial-tulsa",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = Sequential()\n",
    "\n",
    "# input: 3+D tensor with shape: batch_shape + (steps, input_dim)\n",
    "# output: 3+D tensor with shape: batch_shape + (new_steps, filters) steps value might have changed due to padding or strides.\n",
    "basic_model = Sequential()\n",
    "basic_model.add(Dense(128, activation='relu', input_shape=X_train_reshaped.shape[1:]))\n",
    "basic_model.add(Conv1D(24, kernel_size=2, activation='relu'))\n",
    "basic_model.add(MaxPool1D(pool_size=2,padding='same'))\n",
    "basic_model.add(Conv1D(32, kernel_size=2, activation='relu'))\n",
    "basic_model.add(MaxPool1D(pool_size=2,padding='same'))\n",
    "basic_model.add(Conv1D(64, kernel_size=3, activation='relu'))\n",
    "basic_model.add(MaxPool1D(pool_size=2,padding='same'))\n",
    "basic_model.add(Conv1D(128, kernel_size=4, activation='relu'))\n",
    "basic_model.add(MaxPool1D(pool_size=2,padding='same'))\n",
    "basic_model.add(Conv1D(256, kernel_size=4, activation='relu'))\n",
    "basic_model.add(MaxPool1D(pool_size=2,padding='same'))\n",
    "basic_model.add(Dropout(0.25))\n",
    "basic_model.add(Flatten())\n",
    "basic_model.add(Dense(128, activation='relu'))\n",
    "basic_model.add(Dropout(0.25))\n",
    "basic_model.add(Dense(64, activation='relu'))\n",
    "basic_model.add(Dense(1, activation='linear'))\n",
    "\n",
    "basic_model.compile(loss='mean_squared_error',optimizer='adam',metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "shared-patent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 100, 128)          5248      \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 99, 24)            6168      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 50, 24)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 49, 32)            1568      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 25, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 23, 64)            6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 9, 128)            32896     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 2, 256)            131328    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 224,633\n",
      "Trainable params: 224,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "basic_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "victorian-genetics",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 1s 1s/step - loss: 476.4699 - mean_squared_error: 476.4699 - val_loss: 590.6874 - val_mean_squared_error: 590.6874\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 590.68738, saving model to train/cnn_weights.hdf5\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 472.5941 - mean_squared_error: 472.5941 - val_loss: 583.1332 - val_mean_squared_error: 583.1332\n",
      "\n",
      "Epoch 00002: val_loss improved from 590.68738 to 583.13318, saving model to train/cnn_weights.hdf5\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 466.8694 - mean_squared_error: 466.8694 - val_loss: 571.2289 - val_mean_squared_error: 571.2289\n",
      "\n",
      "Epoch 00003: val_loss improved from 583.13318 to 571.22894, saving model to train/cnn_weights.hdf5\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 457.6703 - mean_squared_error: 457.6703 - val_loss: 552.4336 - val_mean_squared_error: 552.4336\n",
      "\n",
      "Epoch 00004: val_loss improved from 571.22894 to 552.43359, saving model to train/cnn_weights.hdf5\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 444.6752 - mean_squared_error: 444.6752 - val_loss: 522.3080 - val_mean_squared_error: 522.3080\n",
      "\n",
      "Epoch 00005: val_loss improved from 552.43359 to 522.30804, saving model to train/cnn_weights.hdf5\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 417.3600 - mean_squared_error: 417.3600 - val_loss: 477.7101 - val_mean_squared_error: 477.7101\n",
      "\n",
      "Epoch 00006: val_loss improved from 522.30804 to 477.71014, saving model to train/cnn_weights.hdf5\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 377.6173 - mean_squared_error: 377.6173 - val_loss: 414.7590 - val_mean_squared_error: 414.7590\n",
      "\n",
      "Epoch 00007: val_loss improved from 477.71014 to 414.75897, saving model to train/cnn_weights.hdf5\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 329.7307 - mean_squared_error: 329.7307 - val_loss: 332.3467 - val_mean_squared_error: 332.3467\n",
      "\n",
      "Epoch 00008: val_loss improved from 414.75897 to 332.34668, saving model to train/cnn_weights.hdf5\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 266.9585 - mean_squared_error: 266.9585 - val_loss: 236.8028 - val_mean_squared_error: 236.8028\n",
      "\n",
      "Epoch 00009: val_loss improved from 332.34668 to 236.80278, saving model to train/cnn_weights.hdf5\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 186.6291 - mean_squared_error: 186.6291 - val_loss: 159.0361 - val_mean_squared_error: 159.0361\n",
      "\n",
      "Epoch 00010: val_loss improved from 236.80278 to 159.03612, saving model to train/cnn_weights.hdf5\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 170.0710 - mean_squared_error: 170.0710 - val_loss: 167.0803 - val_mean_squared_error: 167.0803\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 159.03612\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 210.8839 - mean_squared_error: 210.8839 - val_loss: 177.9721 - val_mean_squared_error: 177.9721\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 159.03612\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 228.3979 - mean_squared_error: 228.3979 - val_loss: 156.4861 - val_mean_squared_error: 156.4861\n",
      "\n",
      "Epoch 00013: val_loss improved from 159.03612 to 156.48613, saving model to train/cnn_weights.hdf5\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 200.7027 - mean_squared_error: 200.7027 - val_loss: 149.9874 - val_mean_squared_error: 149.9874\n",
      "\n",
      "Epoch 00014: val_loss improved from 156.48613 to 149.98735, saving model to train/cnn_weights.hdf5\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 169.3525 - mean_squared_error: 169.3525 - val_loss: 168.1850 - val_mean_squared_error: 168.1850\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 149.98735\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 150.6144 - mean_squared_error: 150.6144 - val_loss: 195.7259 - val_mean_squared_error: 195.7259\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 149.98735\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 163.4889 - mean_squared_error: 163.4889 - val_loss: 218.1639 - val_mean_squared_error: 218.1639\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 149.98735\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 183.4296 - mean_squared_error: 183.4296 - val_loss: 230.8336 - val_mean_squared_error: 230.8336\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 149.98735\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 186.0268 - mean_squared_error: 186.0268 - val_loss: 232.5355 - val_mean_squared_error: 232.5355\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 149.98735\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 186.2844 - mean_squared_error: 186.2844 - val_loss: 224.7279 - val_mean_squared_error: 224.7279\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 149.98735\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 183.5542 - mean_squared_error: 183.5542 - val_loss: 209.2578 - val_mean_squared_error: 209.2578\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 149.98735\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 177.8190 - mean_squared_error: 177.8190 - val_loss: 189.9824 - val_mean_squared_error: 189.9824\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 149.98735\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 152.7130 - mean_squared_error: 152.7130 - val_loss: 170.3982 - val_mean_squared_error: 170.3982\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 149.98735\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 148.7947 - mean_squared_error: 148.7947 - val_loss: 155.7579 - val_mean_squared_error: 155.7579\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 149.98735\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 166.1079 - mean_squared_error: 166.1079 - val_loss: 148.9925 - val_mean_squared_error: 148.9925\n",
      "\n",
      "Epoch 00025: val_loss improved from 149.98735 to 148.99252, saving model to train/cnn_weights.hdf5\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 175.2858 - mean_squared_error: 175.2858 - val_loss: 148.2188 - val_mean_squared_error: 148.2188\n",
      "\n",
      "Epoch 00026: val_loss improved from 148.99252 to 148.21883, saving model to train/cnn_weights.hdf5\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 153.5940 - mean_squared_error: 153.5940 - val_loss: 148.2680 - val_mean_squared_error: 148.2680\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 148.21883\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 172.8439 - mean_squared_error: 172.8439 - val_loss: 148.1336 - val_mean_squared_error: 148.1336\n",
      "\n",
      "Epoch 00028: val_loss improved from 148.21883 to 148.13362, saving model to train/cnn_weights.hdf5\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 176.8275 - mean_squared_error: 176.8275 - val_loss: 150.9704 - val_mean_squared_error: 150.9704\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 148.13362\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 153.8551 - mean_squared_error: 153.8551 - val_loss: 158.0867 - val_mean_squared_error: 158.0867\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 148.13362\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 160.3618 - mean_squared_error: 160.3618 - val_loss: 167.8452 - val_mean_squared_error: 167.8452\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 148.13362\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 147.3863 - mean_squared_error: 147.3863 - val_loss: 176.4130 - val_mean_squared_error: 176.4130\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 148.13362\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 159.0835 - mean_squared_error: 159.0835 - val_loss: 181.4939 - val_mean_squared_error: 181.4939\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 148.13362\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 162.7060 - mean_squared_error: 162.7060 - val_loss: 182.3036 - val_mean_squared_error: 182.3036\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 148.13362\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 163.3454 - mean_squared_error: 163.3454 - val_loss: 179.1559 - val_mean_squared_error: 179.1559\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 148.13362\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 151.8208 - mean_squared_error: 151.8208 - val_loss: 172.4544 - val_mean_squared_error: 172.4544\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 148.13362\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 147.1752 - mean_squared_error: 147.1752 - val_loss: 164.3282 - val_mean_squared_error: 164.3282\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 148.13362\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 149.7873 - mean_squared_error: 149.7873 - val_loss: 157.3431 - val_mean_squared_error: 157.3431\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 148.13362\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 151.0135 - mean_squared_error: 151.0135 - val_loss: 152.5219 - val_mean_squared_error: 152.5219\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 148.13362\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 153.4823 - mean_squared_error: 153.4823 - val_loss: 150.0888 - val_mean_squared_error: 150.0888\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 148.13362\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 93ms/step - loss: 158.9483 - mean_squared_error: 158.9483 - val_loss: 149.4528 - val_mean_squared_error: 149.4528\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 148.13362\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 156.0130 - mean_squared_error: 156.0130 - val_loss: 150.4622 - val_mean_squared_error: 150.4622\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 148.13362\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 147.0052 - mean_squared_error: 147.0052 - val_loss: 153.4008 - val_mean_squared_error: 153.4008\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 148.13362\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 155.3778 - mean_squared_error: 155.3778 - val_loss: 157.7946 - val_mean_squared_error: 157.7946\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 148.13362\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 153.1866 - mean_squared_error: 153.1866 - val_loss: 162.6199 - val_mean_squared_error: 162.6199\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 148.13362\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 158.9613 - mean_squared_error: 158.9613 - val_loss: 167.0731 - val_mean_squared_error: 167.0731\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 148.13362\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 158.1182 - mean_squared_error: 158.1182 - val_loss: 168.9618 - val_mean_squared_error: 168.9618\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 148.13362\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 151.4367 - mean_squared_error: 151.4367 - val_loss: 168.3671 - val_mean_squared_error: 168.3671\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 148.13362\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 150.4939 - mean_squared_error: 150.4939 - val_loss: 164.6285 - val_mean_squared_error: 164.6285\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 148.13362\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 169.2346 - mean_squared_error: 169.2346 - val_loss: 161.0352 - val_mean_squared_error: 161.0352\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 148.13362\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 153.0666 - mean_squared_error: 153.0666 - val_loss: 157.2211 - val_mean_squared_error: 157.2211\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 148.13362\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 144.0755 - mean_squared_error: 144.0755 - val_loss: 153.5419 - val_mean_squared_error: 153.5419\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 148.13362\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 169.8741 - mean_squared_error: 169.8741 - val_loss: 151.9257 - val_mean_squared_error: 151.9257\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 148.13362\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 146.6761 - mean_squared_error: 146.6761 - val_loss: 151.7902 - val_mean_squared_error: 151.7902\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 148.13362\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 151.9258 - mean_squared_error: 151.9258 - val_loss: 152.7929 - val_mean_squared_error: 152.7929\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 148.13362\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 155.4242 - mean_squared_error: 155.4242 - val_loss: 154.5886 - val_mean_squared_error: 154.5886\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 148.13362\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 155.1297 - mean_squared_error: 155.1297 - val_loss: 155.6906 - val_mean_squared_error: 155.6906\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 148.13362\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 157.0815 - mean_squared_error: 157.0815 - val_loss: 157.1618 - val_mean_squared_error: 157.1618\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 148.13362\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 154.1414 - mean_squared_error: 154.1414 - val_loss: 158.3322 - val_mean_squared_error: 158.3322\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 148.13362\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 144.9004 - mean_squared_error: 144.9004 - val_loss: 157.2821 - val_mean_squared_error: 157.2821\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 148.13362\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 146.2224 - mean_squared_error: 146.2224 - val_loss: 155.0111 - val_mean_squared_error: 155.0111\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 148.13362\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 155.6668 - mean_squared_error: 155.6668 - val_loss: 153.7548 - val_mean_squared_error: 153.7548\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 148.13362\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 158.5039 - mean_squared_error: 158.5039 - val_loss: 153.2889 - val_mean_squared_error: 153.2889\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 148.13362\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 165.4850 - mean_squared_error: 165.4850 - val_loss: 153.7115 - val_mean_squared_error: 153.7115\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 148.13362\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 144.0311 - mean_squared_error: 144.0311 - val_loss: 153.8632 - val_mean_squared_error: 153.8632\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 148.13362\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 141.5420 - mean_squared_error: 141.5420 - val_loss: 153.4047 - val_mean_squared_error: 153.4047\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 148.13362\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 145.2249 - mean_squared_error: 145.2249 - val_loss: 151.5785 - val_mean_squared_error: 151.5785\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 148.13362\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 161.4240 - mean_squared_error: 161.4240 - val_loss: 151.4150 - val_mean_squared_error: 151.4150\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 148.13362\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 141.8091 - mean_squared_error: 141.8091 - val_loss: 150.6119 - val_mean_squared_error: 150.6119\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 148.13362\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 154.6313 - mean_squared_error: 154.6313 - val_loss: 151.2755 - val_mean_squared_error: 151.2755\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 148.13362\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 147.3560 - mean_squared_error: 147.3560 - val_loss: 153.0193 - val_mean_squared_error: 153.0193\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 148.13362\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 138.5622 - mean_squared_error: 138.5622 - val_loss: 153.7738 - val_mean_squared_error: 153.7738\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 148.13362\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 144.5917 - mean_squared_error: 144.5917 - val_loss: 153.5882 - val_mean_squared_error: 153.5882\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 148.13362\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 151.7352 - mean_squared_error: 151.7352 - val_loss: 153.1292 - val_mean_squared_error: 153.1292\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 148.13362\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 146.0786 - mean_squared_error: 146.0786 - val_loss: 151.0047 - val_mean_squared_error: 151.0047\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 148.13362\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 138.8288 - mean_squared_error: 138.8288 - val_loss: 148.7737 - val_mean_squared_error: 148.7737\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 148.13362\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 152.9808 - mean_squared_error: 152.9808 - val_loss: 145.4912 - val_mean_squared_error: 145.4912\n",
      "\n",
      "Epoch 00077: val_loss improved from 148.13362 to 145.49123, saving model to train/cnn_weights.hdf5\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 142.7870 - mean_squared_error: 142.7870 - val_loss: 143.7951 - val_mean_squared_error: 143.7951\n",
      "\n",
      "Epoch 00078: val_loss improved from 145.49123 to 143.79507, saving model to train/cnn_weights.hdf5\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 129.5147 - mean_squared_error: 129.5147 - val_loss: 142.0339 - val_mean_squared_error: 142.0339\n",
      "\n",
      "Epoch 00079: val_loss improved from 143.79507 to 142.03394, saving model to train/cnn_weights.hdf5\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 151.9771 - mean_squared_error: 151.9771 - val_loss: 143.0080 - val_mean_squared_error: 143.0080\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 142.03394\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 141.1003 - mean_squared_error: 141.1003 - val_loss: 146.4631 - val_mean_squared_error: 146.4631\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 142.03394\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 139.2102 - mean_squared_error: 139.2102 - val_loss: 149.6792 - val_mean_squared_error: 149.6792\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 142.03394\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 131.4591 - mean_squared_error: 131.4591 - val_loss: 147.6358 - val_mean_squared_error: 147.6358\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 142.03394\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 134.7450 - mean_squared_error: 134.7450 - val_loss: 142.9197 - val_mean_squared_error: 142.9197\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 142.03394\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 148.2054 - mean_squared_error: 148.2054 - val_loss: 140.5338 - val_mean_squared_error: 140.5338\n",
      "\n",
      "Epoch 00085: val_loss improved from 142.03394 to 140.53377, saving model to train/cnn_weights.hdf5\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 138.2576 - mean_squared_error: 138.2576 - val_loss: 138.3790 - val_mean_squared_error: 138.3790\n",
      "\n",
      "Epoch 00086: val_loss improved from 140.53377 to 138.37900, saving model to train/cnn_weights.hdf5\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 124.8138 - mean_squared_error: 124.8138 - val_loss: 137.7107 - val_mean_squared_error: 137.7107\n",
      "\n",
      "Epoch 00087: val_loss improved from 138.37900 to 137.71072, saving model to train/cnn_weights.hdf5\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 158.0343 - mean_squared_error: 158.0343 - val_loss: 141.4048 - val_mean_squared_error: 141.4048\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 137.71072\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 144.9818 - mean_squared_error: 144.9818 - val_loss: 143.9045 - val_mean_squared_error: 143.9045\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 137.71072\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 138.8005 - mean_squared_error: 138.8005 - val_loss: 143.7783 - val_mean_squared_error: 143.7783\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 137.71072\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 161.5760 - mean_squared_error: 161.5760 - val_loss: 143.4528 - val_mean_squared_error: 143.4528\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 137.71072\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 145.5771 - mean_squared_error: 145.5771 - val_loss: 141.4915 - val_mean_squared_error: 141.4915\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 137.71072\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 138.3531 - mean_squared_error: 138.3531 - val_loss: 136.8346 - val_mean_squared_error: 136.8346\n",
      "\n",
      "Epoch 00093: val_loss improved from 137.71072 to 136.83463, saving model to train/cnn_weights.hdf5\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 124.8252 - mean_squared_error: 124.8252 - val_loss: 131.6661 - val_mean_squared_error: 131.6661\n",
      "\n",
      "Epoch 00094: val_loss improved from 136.83463 to 131.66606, saving model to train/cnn_weights.hdf5\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 141.9289 - mean_squared_error: 141.9289 - val_loss: 130.8556 - val_mean_squared_error: 130.8556\n",
      "\n",
      "Epoch 00095: val_loss improved from 131.66606 to 130.85564, saving model to train/cnn_weights.hdf5\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 145.1924 - mean_squared_error: 145.1924 - val_loss: 136.3720 - val_mean_squared_error: 136.3720\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 130.85564\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 125.2103 - mean_squared_error: 125.2103 - val_loss: 136.7680 - val_mean_squared_error: 136.7680\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 130.85564\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 138.8881 - mean_squared_error: 138.8881 - val_loss: 135.1111 - val_mean_squared_error: 135.1111\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 130.85564\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 150.6139 - mean_squared_error: 150.6139 - val_loss: 129.8838 - val_mean_squared_error: 129.8838\n",
      "\n",
      "Epoch 00099: val_loss improved from 130.85564 to 129.88382, saving model to train/cnn_weights.hdf5\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 134.5349 - mean_squared_error: 134.5349 - val_loss: 124.3301 - val_mean_squared_error: 124.3301\n",
      "\n",
      "Epoch 00100: val_loss improved from 129.88382 to 124.33012, saving model to train/cnn_weights.hdf5\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 136.8769 - mean_squared_error: 136.8769 - val_loss: 120.0581 - val_mean_squared_error: 120.0581\n",
      "\n",
      "Epoch 00101: val_loss improved from 124.33012 to 120.05809, saving model to train/cnn_weights.hdf5\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 146.2276 - mean_squared_error: 146.2276 - val_loss: 123.3510 - val_mean_squared_error: 123.3510\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 120.05809\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 137.6136 - mean_squared_error: 137.6136 - val_loss: 126.9066 - val_mean_squared_error: 126.9066\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 120.05809\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 128.0847 - mean_squared_error: 128.0847 - val_loss: 123.0871 - val_mean_squared_error: 123.0871\n",
      "\n",
      "Epoch 00104: val_loss did not improve from 120.05809\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 130.0183 - mean_squared_error: 130.0183 - val_loss: 114.9284 - val_mean_squared_error: 114.9284\n",
      "\n",
      "Epoch 00105: val_loss improved from 120.05809 to 114.92844, saving model to train/cnn_weights.hdf5\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 134.0780 - mean_squared_error: 134.0780 - val_loss: 114.0897 - val_mean_squared_error: 114.0897\n",
      "\n",
      "Epoch 00106: val_loss improved from 114.92844 to 114.08972, saving model to train/cnn_weights.hdf5\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 114.6229 - mean_squared_error: 114.6229 - val_loss: 110.8854 - val_mean_squared_error: 110.8854\n",
      "\n",
      "Epoch 00107: val_loss improved from 114.08972 to 110.88544, saving model to train/cnn_weights.hdf5\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 116.2386 - mean_squared_error: 116.2386 - val_loss: 107.7352 - val_mean_squared_error: 107.7352\n",
      "\n",
      "Epoch 00108: val_loss improved from 110.88544 to 107.73515, saving model to train/cnn_weights.hdf5\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 123.2830 - mean_squared_error: 123.2830 - val_loss: 107.7507 - val_mean_squared_error: 107.7507\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 107.73515\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 119.9960 - mean_squared_error: 119.9960 - val_loss: 102.1837 - val_mean_squared_error: 102.1837\n",
      "\n",
      "Epoch 00110: val_loss improved from 107.73515 to 102.18373, saving model to train/cnn_weights.hdf5\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 123.2448 - mean_squared_error: 123.2448 - val_loss: 103.5246 - val_mean_squared_error: 103.5246\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 102.18373\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 106.9977 - mean_squared_error: 106.9977 - val_loss: 105.7676 - val_mean_squared_error: 105.7676\n",
      "\n",
      "Epoch 00112: val_loss did not improve from 102.18373\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 129.0664 - mean_squared_error: 129.0664 - val_loss: 103.2742 - val_mean_squared_error: 103.2742\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 102.18373\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 111.5012 - mean_squared_error: 111.5012 - val_loss: 90.3149 - val_mean_squared_error: 90.3149\n",
      "\n",
      "Epoch 00114: val_loss improved from 102.18373 to 90.31486, saving model to train/cnn_weights.hdf5\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 119.1263 - mean_squared_error: 119.1263 - val_loss: 82.1563 - val_mean_squared_error: 82.1563\n",
      "\n",
      "Epoch 00115: val_loss improved from 90.31486 to 82.15629, saving model to train/cnn_weights.hdf5\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 115.5752 - mean_squared_error: 115.5752 - val_loss: 92.5871 - val_mean_squared_error: 92.5871\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 82.15629\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 146ms/step - loss: 104.0602 - mean_squared_error: 104.0602 - val_loss: 89.8744 - val_mean_squared_error: 89.8744\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 82.15629\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 97.4442 - mean_squared_error: 97.4442 - val_loss: 74.6145 - val_mean_squared_error: 74.6145\n",
      "\n",
      "Epoch 00118: val_loss improved from 82.15629 to 74.61446, saving model to train/cnn_weights.hdf5\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 104.1201 - mean_squared_error: 104.1201 - val_loss: 76.1803 - val_mean_squared_error: 76.1803\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 74.61446\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 98.0309 - mean_squared_error: 98.0309 - val_loss: 74.4557 - val_mean_squared_error: 74.4557\n",
      "\n",
      "Epoch 00120: val_loss improved from 74.61446 to 74.45573, saving model to train/cnn_weights.hdf5\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 108.3085 - mean_squared_error: 108.3085 - val_loss: 80.1002 - val_mean_squared_error: 80.1002\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 74.45573\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 104.7422 - mean_squared_error: 104.7422 - val_loss: 66.2201 - val_mean_squared_error: 66.2201\n",
      "\n",
      "Epoch 00122: val_loss improved from 74.45573 to 66.22007, saving model to train/cnn_weights.hdf5\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 94.7921 - mean_squared_error: 94.7921 - val_loss: 68.4080 - val_mean_squared_error: 68.4080\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 66.22007\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 88.6524 - mean_squared_error: 88.6524 - val_loss: 71.3159 - val_mean_squared_error: 71.3159\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 66.22007\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 91.2541 - mean_squared_error: 91.2541 - val_loss: 60.6910 - val_mean_squared_error: 60.6910\n",
      "\n",
      "Epoch 00125: val_loss improved from 66.22007 to 60.69097, saving model to train/cnn_weights.hdf5\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 92.4839 - mean_squared_error: 92.4839 - val_loss: 68.8853 - val_mean_squared_error: 68.8853\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 60.69097\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 87.7520 - mean_squared_error: 87.7520 - val_loss: 68.5293 - val_mean_squared_error: 68.5293\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 60.69097\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 98.3110 - mean_squared_error: 98.3110 - val_loss: 59.2769 - val_mean_squared_error: 59.2769\n",
      "\n",
      "Epoch 00128: val_loss improved from 60.69097 to 59.27688, saving model to train/cnn_weights.hdf5\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 100.6179 - mean_squared_error: 100.6179 - val_loss: 111.7586 - val_mean_squared_error: 111.7586\n",
      "\n",
      "Epoch 00129: val_loss did not improve from 59.27688\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 117.1312 - mean_squared_error: 117.1312 - val_loss: 65.5640 - val_mean_squared_error: 65.5640\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 59.27688\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 98.4905 - mean_squared_error: 98.4905 - val_loss: 65.9538 - val_mean_squared_error: 65.9538\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 59.27688\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 111.2810 - mean_squared_error: 111.2810 - val_loss: 80.9103 - val_mean_squared_error: 80.9103\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 59.27688\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 91.9526 - mean_squared_error: 91.9526 - val_loss: 93.4077 - val_mean_squared_error: 93.4077\n",
      "\n",
      "Epoch 00133: val_loss did not improve from 59.27688\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 116.7240 - mean_squared_error: 116.7240 - val_loss: 65.3065 - val_mean_squared_error: 65.3065\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 59.27688\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 82.6175 - mean_squared_error: 82.6175 - val_loss: 63.4017 - val_mean_squared_error: 63.4017\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 59.27688\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 84.4808 - mean_squared_error: 84.4808 - val_loss: 67.0529 - val_mean_squared_error: 67.0529\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 59.27688\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 104.9961 - mean_squared_error: 104.9961 - val_loss: 92.8577 - val_mean_squared_error: 92.8577\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 59.27688\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 96.1677 - mean_squared_error: 96.1677 - val_loss: 79.3271 - val_mean_squared_error: 79.3271\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 59.27688\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 93.9358 - mean_squared_error: 93.9358 - val_loss: 63.9368 - val_mean_squared_error: 63.9368\n",
      "\n",
      "Epoch 00139: val_loss did not improve from 59.27688\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 92.8292 - mean_squared_error: 92.8292 - val_loss: 65.1945 - val_mean_squared_error: 65.1945\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 59.27688\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 91.6051 - mean_squared_error: 91.6051 - val_loss: 79.0175 - val_mean_squared_error: 79.0175\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 59.27688\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 98.3991 - mean_squared_error: 98.3991 - val_loss: 80.9475 - val_mean_squared_error: 80.9475\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 59.27688\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 96.5595 - mean_squared_error: 96.5595 - val_loss: 64.5628 - val_mean_squared_error: 64.5628\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 59.27688\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 81.9813 - mean_squared_error: 81.9813 - val_loss: 60.9497 - val_mean_squared_error: 60.9497\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 59.27688\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 93.9783 - mean_squared_error: 93.9783 - val_loss: 75.4756 - val_mean_squared_error: 75.4756\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 59.27688\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 79.2082 - mean_squared_error: 79.2082 - val_loss: 80.3998 - val_mean_squared_error: 80.3998\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 59.27688\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 100.5736 - mean_squared_error: 100.5736 - val_loss: 64.9683 - val_mean_squared_error: 64.9683\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 59.27688\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 90.2211 - mean_squared_error: 90.2211 - val_loss: 60.7653 - val_mean_squared_error: 60.7653\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 59.27688\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 104.2240 - mean_squared_error: 104.2240 - val_loss: 74.4929 - val_mean_squared_error: 74.4929\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 59.27688\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 81.8207 - mean_squared_error: 81.8207 - val_loss: 82.9328 - val_mean_squared_error: 82.9328\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 59.27688\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 162ms/step - loss: 85.4663 - mean_squared_error: 85.4663 - val_loss: 66.5045 - val_mean_squared_error: 66.5045\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 59.27688\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 82.5443 - mean_squared_error: 82.5443 - val_loss: 63.4137 - val_mean_squared_error: 63.4137\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 59.27688\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 94.2039 - mean_squared_error: 94.2039 - val_loss: 69.2004 - val_mean_squared_error: 69.2004\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 59.27688\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 89.5313 - mean_squared_error: 89.5313 - val_loss: 81.9408 - val_mean_squared_error: 81.9408\n",
      "\n",
      "Epoch 00154: val_loss did not improve from 59.27688\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 86.8661 - mean_squared_error: 86.8661 - val_loss: 66.5689 - val_mean_squared_error: 66.5689\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 59.27688\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 87.6182 - mean_squared_error: 87.6182 - val_loss: 65.3227 - val_mean_squared_error: 65.3227\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 59.27688\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 89.1330 - mean_squared_error: 89.1330 - val_loss: 69.5166 - val_mean_squared_error: 69.5166\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 59.27688\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 84.6872 - mean_squared_error: 84.6872 - val_loss: 70.5883 - val_mean_squared_error: 70.5883\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 59.27688\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 86.9582 - mean_squared_error: 86.9582 - val_loss: 67.4090 - val_mean_squared_error: 67.4090\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 59.27688\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 71.0359 - mean_squared_error: 71.0359 - val_loss: 63.8795 - val_mean_squared_error: 63.8795\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 59.27688\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 78.8443 - mean_squared_error: 78.8443 - val_loss: 65.0942 - val_mean_squared_error: 65.0942\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 59.27688\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 92.0411 - mean_squared_error: 92.0411 - val_loss: 74.8419 - val_mean_squared_error: 74.8419\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 59.27688\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 83.9527 - mean_squared_error: 83.9527 - val_loss: 68.0898 - val_mean_squared_error: 68.0898\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 59.27688\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 82.1422 - mean_squared_error: 82.1422 - val_loss: 63.5192 - val_mean_squared_error: 63.5192\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 59.27688\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 94.2907 - mean_squared_error: 94.2907 - val_loss: 83.8128 - val_mean_squared_error: 83.8128\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 59.27688\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 84.0902 - mean_squared_error: 84.0902 - val_loss: 75.4303 - val_mean_squared_error: 75.4303\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 59.27688\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 88.1464 - mean_squared_error: 88.1464 - val_loss: 62.2932 - val_mean_squared_error: 62.2932\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 59.27688\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 83.1107 - mean_squared_error: 83.1107 - val_loss: 65.6673 - val_mean_squared_error: 65.6673\n",
      "\n",
      "Epoch 00168: val_loss did not improve from 59.27688\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 77.0104 - mean_squared_error: 77.0104 - val_loss: 77.6681 - val_mean_squared_error: 77.6681\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 59.27688\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 90.0620 - mean_squared_error: 90.0620 - val_loss: 75.5980 - val_mean_squared_error: 75.5980\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 59.27688\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 83.5462 - mean_squared_error: 83.5462 - val_loss: 62.3949 - val_mean_squared_error: 62.3949\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 59.27688\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 85.7638 - mean_squared_error: 85.7638 - val_loss: 62.9809 - val_mean_squared_error: 62.9809\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 59.27688\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 80.0907 - mean_squared_error: 80.0907 - val_loss: 84.4221 - val_mean_squared_error: 84.4221\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 59.27688\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 87.5448 - mean_squared_error: 87.5448 - val_loss: 87.8514 - val_mean_squared_error: 87.8514\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 59.27688\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 87.2592 - mean_squared_error: 87.2592 - val_loss: 63.0391 - val_mean_squared_error: 63.0391\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 59.27688\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 87.7665 - mean_squared_error: 87.7665 - val_loss: 62.7134 - val_mean_squared_error: 62.7134\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 59.27688\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 83.3452 - mean_squared_error: 83.3452 - val_loss: 83.2622 - val_mean_squared_error: 83.2622\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 59.27688\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 89.2986 - mean_squared_error: 89.2986 - val_loss: 78.5801 - val_mean_squared_error: 78.5801\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 59.27688\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 92ms/step - loss: 82.5505 - mean_squared_error: 82.5505 - val_loss: 65.3548 - val_mean_squared_error: 65.3548\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 59.27688\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 69.0216 - mean_squared_error: 69.0216 - val_loss: 62.7216 - val_mean_squared_error: 62.7216\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 59.27688\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 80.1958 - mean_squared_error: 80.1958 - val_loss: 81.7490 - val_mean_squared_error: 81.7490\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 59.27688\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 86.0084 - mean_squared_error: 86.0084 - val_loss: 79.6385 - val_mean_squared_error: 79.6385\n",
      "\n",
      "Epoch 00182: val_loss did not improve from 59.27688\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 81.0271 - mean_squared_error: 81.0271 - val_loss: 67.0016 - val_mean_squared_error: 67.0016\n",
      "\n",
      "Epoch 00183: val_loss did not improve from 59.27688\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 77.5884 - mean_squared_error: 77.5884 - val_loss: 65.2342 - val_mean_squared_error: 65.2342\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 59.27688\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 149ms/step - loss: 77.0358 - mean_squared_error: 77.0358 - val_loss: 69.2261 - val_mean_squared_error: 69.2261\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 59.27688\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 87.1141 - mean_squared_error: 87.1141 - val_loss: 70.6775 - val_mean_squared_error: 70.6775\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 59.27688\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 76.3208 - mean_squared_error: 76.3208 - val_loss: 64.7911 - val_mean_squared_error: 64.7911\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 59.27688\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 75.4713 - mean_squared_error: 75.4713 - val_loss: 67.5863 - val_mean_squared_error: 67.5863\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 59.27688\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 75.4179 - mean_squared_error: 75.4179 - val_loss: 77.5532 - val_mean_squared_error: 77.5532\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 59.27688\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 80.4542 - mean_squared_error: 80.4542 - val_loss: 73.5718 - val_mean_squared_error: 73.5718\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 59.27688\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 77.0639 - mean_squared_error: 77.0639 - val_loss: 64.8000 - val_mean_squared_error: 64.8000\n",
      "\n",
      "Epoch 00191: val_loss did not improve from 59.27688\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 75.2527 - mean_squared_error: 75.2527 - val_loss: 69.6661 - val_mean_squared_error: 69.6661\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 59.27688\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 77.0030 - mean_squared_error: 77.0030 - val_loss: 70.1302 - val_mean_squared_error: 70.1302\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 59.27688\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 68.5886 - mean_squared_error: 68.5886 - val_loss: 65.0199 - val_mean_squared_error: 65.0199\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 59.27688\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 80.7880 - mean_squared_error: 80.7880 - val_loss: 69.4581 - val_mean_squared_error: 69.4581\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 59.27688\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 66.7937 - mean_squared_error: 66.7937 - val_loss: 94.9524 - val_mean_squared_error: 94.9524\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 59.27688\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 143ms/step - loss: 90.3446 - mean_squared_error: 90.3446 - val_loss: 77.0644 - val_mean_squared_error: 77.0644\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 59.27688\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 76.8778 - mean_squared_error: 76.8778 - val_loss: 70.5525 - val_mean_squared_error: 70.5525\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 59.27688\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 90.7663 - mean_squared_error: 90.7663 - val_loss: 75.3411 - val_mean_squared_error: 75.3411\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 59.27688\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 80ms/step - loss: 72.6694 - mean_squared_error: 72.6694 - val_loss: 99.7090 - val_mean_squared_error: 99.7090\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 59.27688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa17bd60430>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os.path as op\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "weights_path = op.join(\"train\", 'cnn_weights.hdf5')\n",
    "basic_model_checkpoint_callback = ModelCheckpoint(filepath=weights_path,\n",
    "                                            monitor='val_loss',\n",
    "                                            mode='auto',\n",
    "                                            save_best_only=True,\n",
    "                                            save_weights_only=True,\n",
    "                                            verbose=True)\n",
    "\n",
    "# Fitting basic_model using basic_model checkpoint callback to find best basic_model which is saved to 'weights'\n",
    "basic_model.fit(X_train_reshaped, y_train, epochs=200, batch_size=100, callbacks=[basic_model_checkpoint_callback], validation_data=(X_test_reshaped, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "continuous-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model.load_weights(weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "instant-matthew",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#callback = EarlyStopping(monitor='val_loss', patience=50)\n",
    "#basic_model.fit(X_train_reshaped, y_train, epochs=100, batch_size=100, validation_data=(X_test_reshaped, y_test))\n",
    "#loss_and_metrics = basic_model.evaluate(X_test_reshaped, y_test, batch_size=100)\n",
    "#for name, metric in zip(basic_model.metrics_names, loss_and_metrics):\n",
    "#    print(name, \":\", metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "pointed-sharing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa17cd3b7f0>]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat = basic_model.predict(X_train_reshaped)\n",
    "plt.plot(y_train, y_hat, linestyle=\"\", marker=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "educated-basketball",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa17ce202b0>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_hat = basic_model.predict(X_test_reshaped)\n",
    "plt.plot(y_test, y_hat, linestyle=\"\", marker=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "spare-bulletin",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bored-killer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5888903555226096"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-pillow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-think",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
